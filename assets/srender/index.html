<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Efficient Camera-Controlled Video Generation of Static Scenes via SparseDiffusion and 3D Rendering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Efficient Camera-Controlled Video Generation of Static Scenes via SparseDiffusion and 3D Rendering</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .video-margin {
      margin-bottom: 20px; 
    }
  </style>
  <style>
  .reduce-space {
    margin-bottom: -100px;  
  }
</style>
</head>
<body>


<section class="hero">
  <div class="reduce-space">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class='main-title'>Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</span></h1>
          <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/jieying-chen-5bb4381a1/">Jieying Chen</a>,</span>
                <span class="author-block">
                  <a href="https://jefequien.github.io/">Jeffrey Hu</a>,</span>
                <span class="author-block">
                  <a href="https://www.eng.cam.ac.uk/profiles/jl221">Joan Lasenby</a>,</span>
                <span class="author-block">
                  <a href="https://ayushtewari.com/">Ayush Tewari</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">University of Cambridge</span>
              </div>
              <div class="is-size-5 paper-link">
                <span class="author-block"><a href="./static/paper.pdf">[Paper PDF]</a></span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="rows is-centered">
        <figure>
          <img src="./static/Teaser.png" alt="Teaser image" width="100%">
        </figure>
    <div class="content has-text-justified">
      <p style="font-size: 14px;">
          Left: Overview of our approach. Given an input image and a camera trajectory, we generates sparse keyframes, reconstructs the 3D scene, and renders the full video efficiently. Right: On average, our method is 37.2 times faster than a history-guided diffusion baseline (HG) when genearting a 400-frame 30-fps video from the DL3DV dataset, achieving real-time performance while maintaining comparable or better video quality.
      </p><br>
 
      </div>
        <div class="content has-text-justified">
          <p>
<b>Abstract</b>: Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex motion. This results in video generation that is more than 35 times faster than the diffusion-based baseline in generating 13 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.
        </div>
        <br> 
      </div>
    <!--/ Abstract. -->
  </div>


  <div class="container is-max-desktop">
  <div class="rows is-centered">
    <div class="row">
      <h2 class="title is-3 has-text-centered">Qualitative Comparisons on the RE10K Dataset <br> (20s @ 10fps, 200 frames)</h2>
      <h2 class="content has-text-justified">
        <p>
        </p>
      </h2>

    </div>
    <h2 class="">
          Below are qualitative comparisons of our method and the History-Guided Video Diffusion (HG) baseline on the RE10K dataset. <br> It is shown that while being over 20 times faster, our method produces videos with comparable visual quality. Noticeably, the videos generated by the baseline method often contain flickering artefacts, while our method generates temporally coherent videos due to the underlying 3D representation and rendering process.
    </h2><br>


    <h2 class="">
         - Examples where the videos by the HG models produces heavy artifact and flickers. While our videos are smoother and artifact-free thanks to the underlying 3D Gaussians.
    </h2><br>
      <div class="row">
        <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
          <source src="videos/RE10K/RE10K_comparison_camera.mp4"
              type="video/mp4">
        </video>
      </div><br>
    
    <h2 class="">
          - Examples where the videos by the HG models have heavy flickering (e.g. the chair, the trees).
    </h2><br>
      <div class="row">
        <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
          <source src="videos/RE10K/RE10K_comparison_flicker.mp4"
              type="video/mp4">
        </video>
      </div><br>


    <h2 class="">
          - More examples.
    </h2><br>
      <div class="row">
        <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
          <source src="videos/RE10K/RE10K_comparison_others.mp4"
              type="video/mp4">
        </video>
      </div>
    
    
    
    
    <!-- <div class="row">
        <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
          <source src="videos/RE10K/RE10K_comparison_with_first.mp4"
              type="video/mp4">
        </video>
      </div>
    </div>
  </div> -->


<br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Qualitative Comparisons on the DL3DV Dataset <br> (20s @ 30fps, 600 frames)</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>

      </div>
      <h2 class="">
            Below are qualitative comparisons of our method and the HG baseline on generating longer high fps videos in the DL3DV dataset. Our method can generate videos with comparable quality more than 40x faster. Due to the excessive frames to interpolate, even though the time gap between keyframes is not bigger, the video generated by the HG model shows more prominent flickering and shaky artifacts. Our method, however, maintains a consistent quality.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/DL3DV_30fps_600frames/DL3DV_30fps_600frames_comparison.mp4"
                type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  
<br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Qualitative Comparisons the on DL3DV Dataset <br> with Different Baselines <br> (20s @ 5fps, 100 frames)</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>
      </div>
      <h2 class="">
            Here we show qualitative comparisons of our method and several baselines. The HG model and Voyager model are video generation models, and the FILM and RIFE models are 2D interpolation models. Voyager model completely fails to generate 20-second videos as it relies on the depth map estimated from the input image. The 2D interpolation methods cannot follow the camera controls for the intermediate frames and show significant morphing effects.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/DL3DV_5fps/DL3DV_5fps_comparison.mp4"
                type="video/mp4">
          </video>
        </div>
      </div>
    </div>


<br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Same Input, Different Scenes</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>
      </div>
      <h2 class="">
            Here we show that our model is capable to generate different scenes from the same input while maintaining high quality.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/Multiple_same/multiple_same_grid.mp4"
                type="video/mp4">
          </video>
        </div>



      </div>
    </div>
  
  <br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Generate Videos Following Different Camera Trajectories</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>

      </div>
      <!-- <h2 class="">
            Here we show that after we generate the 3D Gaussian and the first video of a scene following a certain camera trajectory, we can render videos of the same scene following an OOD camera trajectory in a few seconds via rendering, saving the time to re-generate keyframes and the 3D Gaussian. The baseline HG model, however, requires to rerun the whole generation process, and fails to generate coherent videos following the new camera trajectory.
      </h2><br>  -->
        <h2 class="">
            Here we show that after we generate the 3D Gaussian and the first video of a scene following a certain camera trajectory, we can render videos of the same scene following other camera trajectories in a few seconds via rendering, saving the time to re-generate keyframes and the 3D Gaussian. The baseline model, however, requires to rerun the whole generation process, which will take hundreds of seconds.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/OOD_trajectory/ood_trajectory_horizontal.mp4"
                type="video/mp4">
          </video>
        </div>



      </div>
    </div>
  

  <!-- <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Qualitative Comparisons on DL3DV dataset 100 frames 5fps, video generation</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>
        <br>
      </div>
      <h2 class="">
            Below are qualitative comparisons of our method and several baselines.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/DL3DV_5fps/DL3DV_5fps_comparison_video_gen.mp4"
                type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Qualitative Comparisons on DL3DV dataset 100 frames 5fps, interpolation</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>
        <br>
      </div>
      <h2 class="">
            Below are qualitative comparisons of our method and several baselines.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/DL3DV_5fps/DL3DV_5fps_comparison_interpolation.mp4"
                type="video/mp4">
          </video>
        </div>
      </div>
    </div> -->

<br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Effect of Different Keyframe Numbers on the Videos Generated</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>
      </div>
      <h2 class="">
            Too few keyframes lead to underdefined scenes and holes in the generated videos, while too many keyframes waste computation and does not improve the video quality by much. An optimal number of keyframes balances detail and efficiency. The results in the "Ours" column are generated with the keyframe number predicted by our keyframe selection model.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/Keyframe/grid_video.mp4"
                type="video/mp4">
          </video>
        </div>



      </div>
    </div>

<br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Effect of Temporal Chunking on the Video Generated</h2>
        <h2 class="content has-text-justified">
          <p>
          </p>
        </h2>

      </div>
      <h2 class="">
            Temporal chunking improves the consistency within each chunks and as a result improves the sharpness of the videos.
      </h2><br> 
      <div class="row">
          <video id="main" class="video-margin" autoplay muted loop playsinline width="100%">
            <source src="videos/Chunking/horizontal_comparison.mp4"
                type="video/mp4">
          </video>
        </div>



      </div>
    </div>

    

</section>

<nav class="navbar is-white" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              Website source based on <a href="https://github.com/nerfies/nerfies.github.io">this source code</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>